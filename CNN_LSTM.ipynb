{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "def is_env_jupyter_notebook():\n",
    "    env = get_ipython().__class__.__name__\n",
    "    # Jupyter\n",
    "    if env == 'ZMQInteractiveShell':\n",
    "        return True\n",
    "    # IPython\n",
    "    elif env == 'TerminalInteractiveShell':\n",
    "        return False\n",
    "    # Other Shell\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_env_jupyter_notebook():\n",
    "    import sys\n",
    "    # jupyter環境で同一階層のモジュールをimportするため\n",
    "    sys.path.append('.')\n",
    "    # jupyter環境で一つ上の階層のモジュールをimportするため\n",
    "#     sys.path.append('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import gc\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "import copy\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.rnn as rnn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from pydicom import dcmread\n",
    "from tqdm import tqdm\n",
    "from pydicom.data import get_testdata_file\n",
    "from resnet.resnet_for_generation_outputdim256_avgpool import generate_model\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pack_sequence, pad_packed_sequence\n",
    "from torch.optim import Adam\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import BertJapaneseTokenizer, BertModel\n",
    "from utils import(\n",
    "    pickle_dump,\n",
    "    pickle_load,\n",
    "    load_vocab,\n",
    "    get_keys_from_value\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T08:02:51.712607Z",
     "start_time": "2022-03-07T08:02:51.067Z"
    }
   },
   "outputs": [],
   "source": [
    "# gpuを使う場合\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T08:02:51.713680Z",
     "start_time": "2022-03-07T08:02:51.070Z"
    }
   },
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 100\n",
    "RESIZE = 256\n",
    "with_cuda = True\n",
    "# CNN Encoder\n",
    "# model_depth,10, 18, 34, 50, 101, 152, 200\n",
    "model_depth=18\n",
    "# BERT\n",
    "bert_hidden_dim = 768\n",
    "max_length = 256\n",
    "# LSTM\n",
    "lstm_input_length = 256\n",
    "# Adam Optimizer\n",
    "lr = 1e-3\n",
    "adam_beta1 = 0.9\n",
    "adam_beta2 = 0.999\n",
    "adam_weight_decay = 0.01\n",
    "# Decoder\n",
    "attention_dim = 512\n",
    "# embed_dim = 512\n",
    "decoder_dim = 256\n",
    "encoder_dim = 256\n",
    "dropout = 0.1\n",
    "alpha_c = 1.0\n",
    "desired_slice =50\n",
    "DA = 128 # AttentionをNeural Networkで計算する際の重み行列のサイズ\n",
    "R = 1 # Attentionを1層重ねてる\n",
    "last_n_token = 5\n",
    "cnn_pretrain_weight = True\n",
    "\n",
    "# 保存場所\n",
    "num = 'hoge_num'\n",
    "\n",
    "# 事前学習させたCNNモデルのディレクトリと番号\n",
    "cnn_dir = 'hoge_cnn_dir'\n",
    "cnn_num = 'hoge_cnn_num'\n",
    "\n",
    "output_path ='/hoge/output/' + num + '/'\n",
    "os.makedirs(output_path, exist_ok=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T08:02:51.714656Z",
     "start_time": "2022-03-07T08:02:51.074Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_path = '/hoge/dataset/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T08:02:51.715660Z",
     "start_time": "2022-03-07T08:02:51.078Z"
    }
   },
   "outputs": [],
   "source": [
    "# 東北大学が公開しているBERTを，所見文で追加事前学習を行なったモデル．\n",
    "JR_BERT_path =  '/hoge/model_path'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T08:02:51.716578Z",
     "start_time": "2022-03-07T08:02:51.082Z"
    }
   },
   "outputs": [],
   "source": [
    "# 事前学習させたCNNモデル\n",
    "cnn_encoder_path = '/hoge/cnn_output' + '/' + cnn_dir + '/' + 'ep' + cnn_num + '_hoge.bin'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習データ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T08:02:51.717733Z",
     "start_time": "2022-03-07T08:02:51.086Z"
    }
   },
   "outputs": [],
   "source": [
    "train_path = dataset_path + 'train/'\n",
    "train_dicom_path_list = pickle_load(train_path + 'hoge_dicom_path_list.pickle')\n",
    "train_one_sentence_radiology_report_list = pickle_load(train_path + 'hoge_radiology_report_list.pickle')\n",
    "train_dicom_file_num_list = pickle_load(train_path + 'hoge_dicom_file_num_list.pickle')\n",
    "train_posi_nega_list = pickle_load(train_path + 'hoge_posi_nega_lung_region_list.pickle')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 画像取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T08:02:51.718655Z",
     "start_time": "2022-03-07T08:02:51.090Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_images_list(series_path_list, wc=-500, ww=700):\n",
    "    images_list = []\n",
    "    total_num = len(series_path_list)\n",
    "    for series_path in tqdm(series_path_list, bar_format=\"{l_bar}{r_bar}\"):\n",
    "        series_images_list = []\n",
    "        accession_images_list = []\n",
    "#         dicom_path_list = glob.glob(series_path)\n",
    "        sorted_dicom_path_list = sortFileByImagePosition(series_path + '/') # 最後の*を除く\n",
    "        \n",
    "        for dicom_path in sorted_dicom_path_list:\n",
    "            ds = pydicom.read_file(dicom_path)\n",
    "            img_array = ds.pixel_array\n",
    "            img_array = transform_to_hu(ds, img_array)\n",
    "            img_array = Windowing(img_array, wc, ww)\n",
    "            img_array = Resize(img_array)\n",
    "            img_list = img_array.tolist()\n",
    "            series_images_list.append(img_list)\n",
    "            \n",
    "        series_images_array = np.array(series_images_list)\n",
    "        # spline補間\n",
    "        resized_series_images_array = resize_volume(series_images_array, desired_slice)\n",
    "        resized_series_images_list = resized_series_images_array.tolist()\n",
    "        accession_images_list.append([resized_series_images_list])\n",
    "        images_list.extend(accession_images_list)\n",
    "        \n",
    "    return images_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T08:02:51.719509Z",
     "start_time": "2022-03-07T08:02:51.094Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# windowingの結果が正しいのか確認するためjpgに変換\n",
    "def dicom2jpg(series_path_list, save_path, wc=-500, ww=1000):\n",
    "    images_list = []\n",
    "    total_num = len(series_path_list)\n",
    "    for series_path in tqdm(series_path_list, bar_format=\"{l_bar}{r_bar}\"):\n",
    "        series_images_list = []\n",
    "        accession_images_list = []\n",
    "        sorted_dicom_path_list = sortFileByImagePosition(series_path + '/') # 最後の*を除く\n",
    "        \n",
    "        for ind,dicom_path in enumerate(sorted_dicom_path_list):\n",
    "            ds = pydicom.read_file(dicom_path)\n",
    "            img_array = ds.pixel_array\n",
    "            img_array = transform_to_hu(ds, img_array)\n",
    "            img_array = Windowing(img_array, wc, ww)\n",
    "            img_array = Resize(img_array)\n",
    "            new_images_array = img_array*256\n",
    "            \n",
    "            jpg_save_path = save_path + str(ind) + '.jpg'\n",
    "            cv2.imwrite(jpg_save_path, new_images_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T08:02:51.720331Z",
     "start_time": "2022-03-07T08:02:51.098Z"
    }
   },
   "outputs": [],
   "source": [
    "# windowingの結果が正しいのか確認するためjpgに変換\n",
    "# スプライン補間した場合\n",
    "def dicom2jpg_spline(series_path_list, save_path, wc=-500, ww=1000):\n",
    "    img_list = []\n",
    "    total_num = len(series_path_list)\n",
    "    for series_path in tqdm(series_path_list, bar_format=\"{l_bar}{r_bar}\"):\n",
    "        series_img_list = []\n",
    "        accession_img_list = []\n",
    "        sorted_dicom_path_list = sortFileByImagePosition(series_path[:-1]) # 最後の*を除く\n",
    "        sorted_dicom_path_list = sortFileByImagePosition(series_path + '/')\n",
    "        \n",
    "        for ind,dicom_path in enumerate(sorted_dicom_path_list):\n",
    "            ds = pydicom.read_file(dicom_path)\n",
    "            img_array = ds.pixel_array\n",
    "            img_array = transform_to_hu(ds, img_array)\n",
    "            img_array = Windowing(img_array, wc, ww)\n",
    "            img_array = Resize(img_array)\n",
    "            new_img_array = img_array*256\n",
    "            new_img_list = new_img_array.tolist()\n",
    "            series_img_list.append(new_img_list)\n",
    "        series_img_array = np.array(series_img_list)\n",
    "        resized_series_img_array = resize_volume(series_img_array, desired_slice)\n",
    "        \n",
    "        for ind, array in enumerate(resized_series_img_array):\n",
    "            jpg_save_path = save_path + str(ind) + '.jpg'\n",
    "            cv2.imwrite(jpg_save_path, array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T08:02:51.721186Z",
     "start_time": "2022-03-07T08:02:51.102Z"
    }
   },
   "outputs": [],
   "source": [
    "def sortFileByImagePosition(dirname):\n",
    "    files = os.listdir(dirname)\n",
    "  \n",
    "    filedic = {}        # ファイル名とImagePositionを代入する辞書\n",
    "    filelist = []       #　ソートされた順でファイル名を代入するリスト\n",
    "  \n",
    "    #　DICOM画像を読み込んでファイル名とImage Positionの辞書を作成\n",
    "    for i, filename in enumerate(files):\n",
    "        ds = pydicom.read_file(dirname + filename)\n",
    "        filedic[filename] = ds.ImagePositionPatient[2]  # 辞書に登録\n",
    "#         print(\"MakeDic>>\",i, filename, filedic[filename])\n",
    "  \n",
    "    #　Sort( Image Positionを降順ソートのときは -x[1]， 昇順のときは x[1] )\n",
    "    fileSortedByImgpos = sorted(filedic.items(), key=lambda x: -x[1])\n",
    "    for i, fname_imgPos in enumerate(fileSortedByImgpos):\n",
    "        dicom_path_list = dirname + str(fname_imgPos[0])\n",
    "        filelist.append(dicom_path_list)\n",
    "#         print(\"Sorted >> \",i ,str(fname_imgPos[0]) + \": \" + str(fname_imgPos[1]))\n",
    "      \n",
    "    return filelist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 画像前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T08:02:51.721935Z",
     "start_time": "2022-03-07T08:02:51.106Z"
    }
   },
   "outputs": [],
   "source": [
    "def Resize(img_array):\n",
    "    \n",
    "    img_array = cv2.resize(img_array, (RESIZE, RESIZE))\n",
    "    img_array = img_array.astype(np.float32)\n",
    "    \n",
    "    return img_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T08:02:51.722743Z",
     "start_time": "2022-03-07T08:02:51.109Z"
    }
   },
   "outputs": [],
   "source": [
    "# 対象領域を見やすくするための関数\n",
    "# 肺領域を見やすくするにはwc,wwがこのくらいの値だといいらしい\n",
    "# window center: -500\n",
    "# window width: 1000\n",
    "# 画像ごとに値があるようだけど，固定値で行う\n",
    "def Windowing(img_array, wc, ww):\n",
    "    img_array =  (img_array - wc + ww/2) / ww\n",
    "    img_array[img_array > 1] = 1\n",
    "    img_array[img_array < 0] = 0\n",
    "    return img_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T08:02:51.723540Z",
     "start_time": "2022-03-07T08:02:51.113Z"
    }
   },
   "outputs": [],
   "source": [
    "# (0028, 1052) Rescale Intercept対策\n",
    "# たまに+1024されている画像がある．それを元に戻す関数\n",
    "# Hounsfield Unit: HU ハウスフィールド単位に戻す\n",
    "# 水:0HU, 空気:-1000HU\n",
    "def transform_to_hu(medical_image, image):\n",
    "    intercept = medical_image.RescaleIntercept\n",
    "    slope = medical_image.RescaleSlope\n",
    "    hu_image = image * slope + intercept\n",
    "\n",
    "    return hu_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T08:02:51.724339Z",
     "start_time": "2022-03-07T08:02:51.117Z"
    }
   },
   "outputs": [],
   "source": [
    "# spline補間\n",
    "def resize_volume(img, desired_slice=100):\n",
    "    \"\"\"Resize across z-axis\"\"\"\n",
    "    # Get current slice\n",
    "    current_slice = img.shape[0]\n",
    "    \n",
    "    # Compute slice factor\n",
    "    slice = current_slice / desired_slice\n",
    "    \n",
    "    slice_factor = 1 / slice\n",
    "    # Resize across z-axis\n",
    "    img = zoom(img, (slice_factor, 1, 1), order=3)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T08:02:51.725149Z",
     "start_time": "2022-03-07T08:02:51.120Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_images_list = get_images_list(train_dicom_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T08:02:51.725965Z",
     "start_time": "2022-03-07T08:02:51.124Z"
    }
   },
   "outputs": [],
   "source": [
    "additional_special_tokens = ['DATE']\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(\n",
    "    JR_BERT_path,\n",
    "    additional_special_tokens=additional_special_tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# captionのidを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T08:02:51.726925Z",
     "start_time": "2022-03-07T08:02:51.128Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_subword(text):\n",
    "    new_text_list = []\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    for word in tokenized_text:\n",
    "        if '#' in word:\n",
    "            new_text_list[-1] += word.replace('#','')\n",
    "        else:\n",
    "            new_text_list.append(word)\n",
    "            \n",
    "    return new_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T08:02:51.727656Z",
     "start_time": "2022-03-07T08:02:51.132Z"
    }
   },
   "outputs": [],
   "source": [
    "train_word_dic = {}\n",
    "train_word_dic['<PAD>'] = 0\n",
    "train_word_dic['<MASK>'] = 1\n",
    "train_word_dic['<START>'] = 2\n",
    "train_word_dic['<END>'] = 3\n",
    "train_word_dic['<DATE>'] = 4\n",
    "train_word_dic['<UNK>'] = 5\n",
    "ind = 6\n",
    "for text in train_one_sentence_radiology_report_list:\n",
    "    remove_subword_list = remove_subword(text)\n",
    "    for word in remove_subword_list:\n",
    "        if word == 'DATE':\n",
    "            continue\n",
    "        if word not in train_word_dic:\n",
    "            train_word_dic[word] = ind\n",
    "            ind += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T08:02:51.728474Z",
     "start_time": "2022-03-07T08:02:51.135Z"
    }
   },
   "outputs": [],
   "source": [
    "def id2word(id_):\n",
    "    return keys_list[id_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T08:02:51.729295Z",
     "start_time": "2022-03-07T08:02:51.139Z"
    }
   },
   "outputs": [],
   "source": [
    "keys_list = list(train_word_dic.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# captionのtargetを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T08:02:51.730140Z",
     "start_time": "2022-03-07T08:02:51.143Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_caption_id(text_list):\n",
    "    ids_list = []\n",
    "    for text in text_list:\n",
    "        id_list = []\n",
    "        id_list.append(train_word_dic['<START>'])\n",
    "        remove_subword_list = remove_subword(text)\n",
    "        for word in remove_subword_list:\n",
    "            if word not in train_word_dic:\n",
    "                if word=='DATE':\n",
    "                    id_list.append(train_word_dic['<DATE>'])\n",
    "                else:\n",
    "                    id_list.append(train_word_dic['<UNK>'])\n",
    "            else:\n",
    "                id_list.append(train_word_dic[word])\n",
    "        id_list.append(train_word_dic['<END>'])\n",
    "        ids_list.append(id_list)\n",
    "    \n",
    "    return ids_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T08:02:51.730982Z",
     "start_time": "2022-03-07T08:02:51.146Z"
    }
   },
   "outputs": [],
   "source": [
    "train_caption_id_list = make_caption_id(train_one_sentence_radiology_report_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# embedding_matrix,input_ids_listを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T08:02:51.731735Z",
     "start_time": "2022-03-07T08:02:51.151Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T08:02:51.732564Z",
     "start_time": "2022-03-07T08:02:51.154Z"
    }
   },
   "outputs": [],
   "source": [
    "bert = BertModel.from_pretrained(JR_BERT_path).to(device)\n",
    "bert.resize_token_embeddings(len(tokenizer))\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T08:02:51.733417Z",
     "start_time": "2022-03-07T08:02:51.158Z"
    }
   },
   "outputs": [],
   "source": [
    "total_subword_num = 0\n",
    "for sentence in train_one_sentence_radiology_report_list:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        sentence,                      # Sentence to encode.\n",
    "        add_special_tokens=False,  # '[CLS]' and '[SEP]'\n",
    "        return_attention_mask=False,   # Construct attn. masks.\n",
    "        #         return_tensors='pt',    # Return pytorch tensors.\n",
    "        return_length=True\n",
    "    )\n",
    "    total_subword_num += encoded_dict['length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T08:02:51.734437Z",
     "start_time": "2022-03-07T08:02:51.162Z"
    }
   },
   "outputs": [],
   "source": [
    "# 0番目はpadding用，　１番目はstart_embeddings用\n",
    "embedding_matrix = np.zeros((total_subword_num+2, 768), dtype=np.float32)\n",
    "embedding_matrix_ind = 2\n",
    "input_ids_list = []\n",
    "\n",
    "for sentence in tqdm(train_one_sentence_radiology_report_list, bar_format=\"{l_bar}{r_bar}\"):\n",
    "    input_id_list = [0] # start_embedding用\n",
    "    prev_last_subword_ind = 0\n",
    "    total_subword_len = 0\n",
    "\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        sentence,                      # Sentence to encode.\n",
    "        add_special_tokens=True,  # '[CLS]' and '[SEP]'\n",
    "        return_attention_mask=True,   # Construct attn. masks.\n",
    "        return_length=True\n",
    "    )\n",
    "    subword_token_list = []\n",
    "    for id_ in encoded_dict['input_ids']:\n",
    "        subword_token_list.append(tokenizer.convert_ids_to_tokens(id_))\n",
    "\n",
    "#     print(subword_token_list)\n",
    "    subword_len = 1\n",
    "    for ind in range(len(subword_token_list)):\n",
    "        if subword_token_list[ind + 2] == '[SEP]':\n",
    "            total_subword_len += subword_len\n",
    "            bert_input_ids = encoded_dict['input_ids'][:total_subword_len+1]\n",
    "            token_type_ids = encoded_dict['token_type_ids'][:total_subword_len+1]\n",
    "            bert_input_ids = torch.tensor([bert_input_ids]).to(device)\n",
    "            token_type_ids = torch.tensor([token_type_ids]).to(device)\n",
    "\n",
    "            bert_outputs = bert(\n",
    "                bert_input_ids,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "            bert_last_hidden_state = bert_outputs.last_hidden_state\n",
    "            subword_embeddings = bert_last_hidden_state[0][prev_last_subword_ind+1:prev_last_subword_ind+subword_len+1]\n",
    "            \n",
    "            embeddings = torch.sum(subword_embeddings,dim=0).tolist()\n",
    "            embedding_matrix[embedding_matrix_ind] = embeddings\n",
    "            input_id_list.append(embedding_matrix_ind)\n",
    "            \n",
    "            embedding_matrix_ind += 1\n",
    "            \n",
    "            break\n",
    "            \n",
    "        if '#' in subword_token_list[ind + 2]:\n",
    "            subword_len += 1\n",
    "        else:\n",
    "            total_subword_len += subword_len\n",
    "            bert_input_ids = encoded_dict['input_ids'][:total_subword_len+1]\n",
    "            token_type_ids = encoded_dict['token_type_ids'][:total_subword_len+1]\n",
    "            bert_input_ids = torch.tensor([bert_input_ids]).to(device)\n",
    "            token_type_ids = torch.tensor([token_type_ids]).to(device)\n",
    "\n",
    "            bert_outputs = bert(\n",
    "                bert_input_ids,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "            bert_last_hidden_state = bert_outputs.last_hidden_state\n",
    "            subword_embeddings = bert_last_hidden_state[0][prev_last_subword_ind+1:prev_last_subword_ind+subword_len+1]\n",
    "            prev_last_subword_ind += subword_len\n",
    "            subword_len = 1\n",
    "\n",
    "            embeddings = torch.sum(subword_embeddings,dim=0).tolist()\n",
    "            embedding_matrix[embedding_matrix_ind] = embeddings\n",
    "            input_id_list.append(embedding_matrix_ind)\n",
    "            \n",
    "            embedding_matrix_ind += 1\n",
    "    \n",
    "    input_ids_list.append(input_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T08:02:51.735331Z",
     "start_time": "2022-03-07T08:02:51.165Z"
    }
   },
   "outputs": [],
   "source": [
    "# -1以上，１未満の一様分布\n",
    "np.random.seed(seed=SEED)\n",
    "start_embeddings = (2 * np.random.rand(768) -1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T08:02:51.736162Z",
     "start_time": "2022-03-07T08:02:51.169Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_matrix[1] = start_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T08:02:51.736925Z",
     "start_time": "2022-03-07T08:02:51.173Z"
    }
   },
   "outputs": [],
   "source": [
    "unk_list = np.zeros(768).tolist()\n",
    "embedding_matrix[0] = unk_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T08:02:51.737775Z",
     "start_time": "2022-03-07T08:02:51.176Z"
    }
   },
   "outputs": [],
   "source": [
    "del bert\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T08:02:51.738604Z",
     "start_time": "2022-03-07T08:02:51.180Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T08:02:51.739376Z",
     "start_time": "2022-03-07T08:02:51.185Z"
    }
   },
   "outputs": [],
   "source": [
    "class Resnet_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dicom_images, bert_output_id, caption_id, dicom_file_num, posi_nega):\n",
    "        self.dicom_images = dicom_images\n",
    "        self.bert_output_id = bert_output_id\n",
    "        self.caption_id = caption_id\n",
    "        self.dicom_file_num = dicom_file_num\n",
    "        self.posi_nega = posi_nega\n",
    "        self.data_num = len(dicom_images)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data_num\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.dicom_images[idx], \\\n",
    "                    torch.tensor(self.bert_output_id[idx]), \\\n",
    "                    torch.tensor(self.caption_id[idx]), \\\n",
    "                    [len(self.caption_id[idx])] ,\\\n",
    "                    self.dicom_file_num[idx] ,\\\n",
    "                    self.posi_nega[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T08:02:51.740049Z",
     "start_time": "2022-03-07T08:02:51.188Z"
    }
   },
   "outputs": [],
   "source": [
    "def my_collate(batch):\n",
    "    resnet_input = [item[0] for item in batch]\n",
    "    bert_output_id = [item[1] for item in batch]\n",
    "    caption_id = [item[2] for item in batch]\n",
    "    caption_len = [item[3] for item in batch]\n",
    "    dicom_file_num = [item[4] for item in batch]\n",
    "    posi_nega = [item[5] for item in batch]\n",
    "    \n",
    "    return resnet_input, bert_output_id, caption_id, caption_len, dicom_file_num, posi_nega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T08:02:51.741865Z",
     "start_time": "2022-03-07T08:02:51.192Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "dataset_train = Resnet_Dataset(\n",
    "    train_images_list,\n",
    "    input_ids_list,\n",
    "    train_caption_id_list,\n",
    "    train_dicom_file_num_list,\n",
    "    train_posi_nega_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T08:02:51.742625Z",
     "start_time": "2022-03-07T08:02:51.196Z"
    }
   },
   "outputs": [],
   "source": [
    "del train_images_list\n",
    "del input_ids_list\n",
    "del train_one_sentence_radiology_report_list\n",
    "del train_caption_id_list\n",
    "del train_dicom_file_num_list\n",
    "del train_posi_nega_list\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T08:02:51.743376Z",
     "start_time": "2022-03-07T08:02:51.199Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "    dataset_train, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    collate_fn=my_collate,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# subwordのembeddingsを足す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T08:02:51.744152Z",
     "start_time": "2022-03-07T08:02:51.204Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def add_subword_embeddings(bert_last_hidden_state, target_id_captions, subword_id_captions, start_embedding):\n",
    "    batch_ind = 0\n",
    "    embeddings = []\n",
    "    for target_id_caption, subword_id_caption in zip(target_id_captions, subword_id_captions):\n",
    "        tokens_embedding = []\n",
    "        total_token_num = 0\n",
    "        tokens_embedding.append(start_embedding)\n",
    "        subword_tokenized_caption = tokenizer.convert_ids_to_tokens(subword_id_caption[1:-1]) # [CLS],[SEP]を無視\n",
    "        for target_id in target_id_caption[1:-1]: # [START],[END]を無視\n",
    "            current_token = ''\n",
    "            subword_num = 0\n",
    "            target_token = get_keys_from_value(train_word_dic, target_id)[0]\n",
    "            \n",
    "            for ind, _ in enumerate(subword_tokenized_caption):\n",
    "                subword_token = subword_tokenized_caption[ind + total_token_num]\n",
    "                if subword_token == '[PAD]':\n",
    "                    break\n",
    "                piece_embedding = bert_last_hidden_state[batch_ind][ind + total_token_num + 1] # [CLS]文を+1\n",
    "\n",
    "                if subword_token == target_token:\n",
    "                    tokens_embedding.append(piece_embedding)\n",
    "                    total_token_num += 1\n",
    "                    break\n",
    "                else:\n",
    "                    subword_num += 1\n",
    "                    \n",
    "                    if current_token == '':\n",
    "                        tokens_embedding.append(piece_embedding)\n",
    "                        current_token += subword_token.replace('#', '')\n",
    "                        \n",
    "                    else:\n",
    "                        tokens_embedding[-1] += piece_embedding\n",
    "                        current_token += subword_token.replace('#', '')\n",
    "                        \n",
    "                        if current_token == target_token:\n",
    "                            total_token_num += subword_num\n",
    "                            break\n",
    "        tokens_embedding = torch.stack(tokens_embedding) # -> tensor([[]])\n",
    "        \n",
    "        packed = pack_sequence([tokens_embedding])\n",
    "        pad_sequence, _ = pad_packed_sequence(packed, batch_first=True, total_length=lstm_input_length)\n",
    "        squeezed_sequence = torch.squeeze(pad_sequence) # (1, lstm_input_length, 768) -> (lstm_input_length, 768)\n",
    "        embeddings.append(squeezed_sequence)\n",
    "        batch_ind += 1\n",
    "        \n",
    "    embeddings = torch.stack(embeddings)\n",
    "    \n",
    "    return embeddings\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T08:02:51.744759Z",
     "start_time": "2022-03-07T08:02:51.207Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def last_n_token_tensor(seq_tensor, length_list, last_n_token):\n",
    "    new_seq_list = []\n",
    "    max_ = seq_tensor.shape[1]\n",
    "    \n",
    "    for seq, len_ in zip(seq_tensor, length_list):\n",
    "        packed = pack_sequence([seq[len_- last_n_token:]])\n",
    "        pad, _ = pad_packed_sequence(packed, batch_first=True, total_length=max_, padding_value=0)\n",
    "        new_seq_list.append(pad[0])\n",
    "    \n",
    "    new_seq_tensor = torch.stack(new_seq_list)\n",
    "    return new_seq_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T08:02:51.745381Z",
     "start_time": "2022-03-07T08:02:51.211Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Classification(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden=encoder_dim, n_classes=2):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(hidden, n_classes)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        encoder_dim = x.size(-1)\n",
    "        # Flatten image\n",
    "        x = x.view(batch_size, -1, encoder_dim)\n",
    "        # global average pooling\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.linear(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T08:02:51.746108Z",
     "start_time": "2022-03-07T08:02:51.215Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class CNN_Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, device, model_depth, encoded_image_size=14, bert_requires_grad=False):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        self.device = device\n",
    "        self.enc_image_size = encoded_image_size\n",
    "\n",
    "        # model_depth,10, 18, 34, 50, 101, 152, 200\n",
    "        self.resnet = generate_model(encoded_image_size, model_depth)\n",
    "\n",
    "    def forward(self, images):\n",
    "        encoder_out = self.resnet(images) # (batch, encoder_dim, encoded_image_size, encoded_image_size)\n",
    "        encoder_out = encoder_out.permute(0, 2, 3, 1) # (batch_size, encoded_image_size, encoded_image_size,  encoder_dim)\n",
    "        return encoder_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T08:02:51.746917Z",
     "start_time": "2022-03-07T08:02:51.220Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class BERT_Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, bert, device, bert_requires_grad=False):\n",
    "        super(BERT_Decoder, self).__init__()\n",
    "\n",
    "        self.bert = bert\n",
    "        # bert_requires_grad=FalseならBERTの重み固定\n",
    "        if not bert_requires_grad:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, captions):\n",
    "        \n",
    "        bert_input_ids = []\n",
    "        attention_masks = []\n",
    "        token_type_ids = []\n",
    "        for sent in captions:\n",
    "            encoded_dict = tokenizer.encode_plus(\n",
    "                sent,                      # Sentence to encode.\n",
    "                add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "                max_length=max_length,           # Pad & truncate all sentences.\n",
    "                pad_to_max_length=True,\n",
    "                return_attention_mask=True,   # Construct attn. masks.\n",
    "                return_tensors='pt',     # Return pytorch tensors.\n",
    "            )\n",
    "            bert_input_ids.append(encoded_dict['input_ids'])\n",
    "            attention_masks.append(encoded_dict['attention_mask'])\n",
    "            token_type_ids.append(encoded_dict['token_type_ids'])\n",
    "        bert_input_ids = torch.cat(bert_input_ids, dim=0).to(self.device)\n",
    "        attention_masks = torch.cat(attention_masks, dim=0).to(self.device)\n",
    "        token_type_ids = torch.cat(token_type_ids, dim=0).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            bert_outputs = self.bert(\n",
    "                bert_input_ids,\n",
    "                attention_mask=attention_masks,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "\n",
    "        return bert_outputs, bert_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T08:02:51.747583Z",
     "start_time": "2022-03-07T08:02:51.223Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention Network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
    "        \"\"\"\n",
    "        :param encoder_dim: feature size of encoded images\n",
    "        :param decoder_dim: size of decoder's RNN\n",
    "        :param attention_dim: size of the attention network\n",
    "        \"\"\"\n",
    "        super(Attention, self).__init__()\n",
    "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # linear layer to transform encoded image\n",
    "        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # linear layer to transform decoder's output\n",
    "        self.full_att = nn.Linear(attention_dim, 1)  # linear layer to calculate values to be softmax-ed\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)  # softmax layer to calculate weights\n",
    "\n",
    "    def forward(self, encoder_out, decoder_hidden):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n",
    "        :param decoder_hidden: previous decoder output, a tensor of dimension (batch_size, decoder_dim)\n",
    "        :return: attention weighted encoding, weights\n",
    "        \"\"\"\n",
    "        att1 = self.encoder_att(encoder_out)  # (batch_size, num_pixels, attention_dim)\n",
    "        att2 = self.decoder_att(decoder_hidden)  # (batch_size, attention_dim)\n",
    "        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)  # (batch_size, num_pixels)\n",
    "        alpha = self.softmax(att)  # (batch_size, num_pixels)\n",
    "        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # (batch_size, encoder_dim)\n",
    "\n",
    "        return attention_weighted_encoding, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T08:02:51.748279Z",
     "start_time": "2022-03-07T08:02:51.228Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        attention_dim,\n",
    "        embed_dim, \n",
    "        decoder_dim, \n",
    "        decoder_vocab_size,\n",
    "        device,\n",
    "        encoder_dim=512, \n",
    "        dropout=0.1,\n",
    "        subword_num=32000\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param attention_dim: size of attention network\n",
    "        :param embed_dim: embedding size\n",
    "        :param decoder_dim: size of decoder's RNN\n",
    "        :param vocab_size: size of vocabulary\n",
    "        :param encoder_dim: feature size of encoded images\n",
    "        :param dropout: dropout\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.device = device\n",
    "\n",
    "        self.vocab_size = decoder_vocab_size\n",
    "        \n",
    "        self.attention = Attention(encoder_dim, decoder_dim, attention_dim).to(self.device)  # attention network\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim + 1, decoder_dim, bias=True)  # decoding LSTMCell\n",
    "        self.init_h = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial hidden state of LSTMCell\n",
    "        self.init_c = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial cell state of LSTMCell\n",
    "\n",
    "        # init_hidden_stateではこっちがいいのかも\n",
    "#         self.init_h = nn.Linear(num_pixels, decoder_dim)  \n",
    "#         self.init_c = nn.Linear(num_pixels, decoder_dim)  \n",
    "        \n",
    "        self.f_beta = nn.Linear(decoder_dim, encoder_dim)  # linear layer to create a sigmoid-activated gate\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc = nn.Linear(decoder_dim, self.vocab_size)  # linear layer to find scores over vocabulary\n",
    "        self.init_weights()  # initialize some layers with the uniform distribution\n",
    "\n",
    "        self.embedding = nn.Embedding(subword_num, embed_dim).to(self.device)\n",
    "        self.init_embeddings(embedding_matrix)\n",
    "        \n",
    "    def init_embeddings(self, weights):\n",
    "        self.embedding.weight = nn.Parameter(torch.from_numpy(weights))\n",
    "        \n",
    "        #トレーニング中，重みを更新させない\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initializes some parameters with values from the uniform distribution, for easier convergence.\n",
    "        \"\"\"\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
    "        \n",
    "    def init_hidden_state(self, encoder_out):\n",
    "        \"\"\"\n",
    "        Creates the initial hidden and cell states for the decoder's LSTM based on the encoded images.\n",
    "        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n",
    "        :return: hidden state, cell state\n",
    "        \"\"\"\n",
    "        # これいいのかな？\n",
    "        # 一つの画像全体を平均させている？\n",
    "        # いいっぽい．やっていることはglobal average pooling\n",
    "        mean_encoder_out = encoder_out.mean(dim=1)\n",
    "        \n",
    "        # 各画像の一致するピクセルごとを平均した方がいいのではないか？\n",
    "        # こっちでも良さそうだけど，上のやつでもいいっぽい\n",
    "#         mean_encoder_out = encoder_out.mean(dim=-1) # (batch, num_pixels)\n",
    "        # これをnum_pixels -> decoder_dimとなる全結合を行えばいいと思う)\n",
    "        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n",
    "        c = self.init_c(mean_encoder_out)\n",
    "        return h, c\n",
    "\n",
    "    def forward(self, encoder_out, bert_output_id, captions_id, caption_lengths, posi_nega):\n",
    "\n",
    "        batch_size = encoder_out.size(0)\n",
    "        encoder_dim = encoder_out.size(-1)\n",
    "        vocab_size = self.vocab_size\n",
    "\n",
    "        # Flatten image\n",
    "        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n",
    "        num_pixels = encoder_out.size(1)\n",
    "\n",
    "        # Sort input data by decreasing lengths; why? apparent below\n",
    "        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n",
    "        \n",
    "        encoder_out = encoder_out[sort_ind]\n",
    "        bert_output_id = bert_output_id[sort_ind]\n",
    "        posi_nega = posi_nega.unsqueeze(dim=-1)\n",
    "        posi_nega = posi_nega[sort_ind].float()\n",
    "\n",
    "        lstm_inputs = self.embedding(bert_output_id)\n",
    "    \n",
    "        pad_caps_sorted = pad_sequence(captions_id, batch_first=True)\n",
    "        pad_caps_sorted = pad_caps_sorted[sort_ind]\n",
    "        \n",
    "        # Initialize LSTM state\n",
    "        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n",
    "\n",
    "        # We won't decode at the <end> position, since we've finished generating as soon as we generate <end>\n",
    "        # So, decoding lengths are actual lengths - 1\n",
    "        decode_lengths = (caption_lengths - 1).tolist()\n",
    "#         print(decode_lengths)\n",
    "\n",
    "        # Create tensors to hold word predicion scores and alphas\n",
    "        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size)#.to(device)\n",
    "        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels)#.to(device)\n",
    "\n",
    "        for t in range(max(decode_lengths)):\n",
    "            # captions_idは降順になっている．\n",
    "            # そのためbatch_size_tだけ，まだ単語が残っているのでhoge[:batch_size_t]としている．\n",
    "            # 効率よく計算するためかな...\n",
    "            batch_size_t = sum([l > t for l in decode_lengths])\n",
    "            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t], h[:batch_size_t])\n",
    "            # attentionあり\n",
    "            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))  # gating scalar, (batch_size_t, encoder_dim)\n",
    "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
    "            # embeddingにgateをconcatするのは少し違和感\n",
    "            # こういうタイプのlstmもあるのかな...\n",
    "            # ちがう，attention重みつきencodingとhを融合させるためかな\n",
    "            h, c = self.decode_step(\n",
    "                torch.cat([posi_nega[:batch_size_t], lstm_inputs[:batch_size_t, t, :], attention_weighted_encoding], dim=1),\n",
    "                (h[:batch_size_t], c[:batch_size_t]))  # (batch_size_t, decoder_dim)\n",
    "            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n",
    "            predictions[:batch_size_t, t, :] = preds\n",
    "            alphas[:batch_size_t, t, :] = alpha\n",
    "\n",
    "        return predictions, pad_caps_sorted, decode_lengths,  sort_ind, alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T08:02:51.749238Z",
     "start_time": "2022-03-07T08:02:51.235Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class CNN_BERT_LSTM_Trainer:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_dataloader: DataLoader,\n",
    "        dev_dataloader: DataLoader = None,\n",
    "        with_cuda: bool = True,\n",
    "        lr: float = 1e-4,\n",
    "        betas=(0.9, 0.999),\n",
    "        weight_decay: float = 0.0,\n",
    "        attention_dim: int = 512,\n",
    "        embed_dim: int = 768,\n",
    "        decoder_dim: int = 512,\n",
    "        decoder_vocab_size: int = 32000,\n",
    "        encoder_dim: int = 512,\n",
    "        dropout: float = 0.1,\n",
    "        output_path=None,\n",
    "        subword_num: int = 32000,\n",
    "        model_depth: int = 18,\n",
    "        last_n_token: int = 5,\n",
    "        cnn_pretrain_weight: bool = False\n",
    "    ):\n",
    "        # set up cuda device\n",
    "        cuda_condition = torch.cuda.is_available() and with_cuda\n",
    "        self.device = torch.device('cuda' if cuda_condition else \"cpu\")\n",
    "\n",
    "        self.cnn_encoder = CNN_Encoder(\n",
    "            self.device, model_depth).to(self.device)\n",
    "        \n",
    "        if cnn_pretrain_weight:\n",
    "            self.cnn_encoder.load_state_dict(torch.load(cnn_encoder_path))\n",
    "\n",
    "        self.classification = Classification(encoder_dim).to(self.device)\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "            attention_dim,\n",
    "            embed_dim,\n",
    "            decoder_dim,\n",
    "            decoder_vocab_size,\n",
    "            self.device,\n",
    "            encoder_dim,\n",
    "            dropout,\n",
    "            subword_num\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.train_dataloader = train_dataloader\n",
    "        if dev_dataloader:\n",
    "            self.dev_dataloader = dev_dataloader\n",
    "\n",
    "        self.cnn_encoder_optimizer = Adam(\n",
    "            self.cnn_encoder.parameters(),\n",
    "            lr=lr,\n",
    "            betas=betas,\n",
    "            weight_decay=weight_decay\n",
    "        )\n",
    "\n",
    "        self.decoder_optimizer = Adam(\n",
    "            self.decoder.parameters(),\n",
    "            lr=lr,\n",
    "            betas=betas,\n",
    "            weight_decay=weight_decay\n",
    "        )\n",
    "\n",
    "        # loss function\n",
    "        self.decoder_criterion = nn.CrossEntropyLoss()\n",
    "        self.encoder_criterion = nn.CrossEntropyLoss()\n",
    "        self.last_n_token_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # prepare save file\n",
    "        acc_locc_train_dic = {'loss': [], 'bleu': [], 'image_binary_acc': [], 'f1': []}\n",
    "        self.output_train_path = output_path+'output_train.pickle'\n",
    "        pickle_dump(acc_locc_train_dic, self.output_train_path)\n",
    "\n",
    "        if dev_dataloader:\n",
    "            acc_locc_dev_dic = {'loss': [], 'bleu': []}\n",
    "            self.output_dev_path = output_path+'output_dev.pickle'\n",
    "            pickle_dump(acc_locc_dev_dic, self.output_dev_path)\n",
    "\n",
    "    def train(self, epoch):\n",
    "\n",
    "        self.cnn_encoder.train()\n",
    "        self.decoder.train()\n",
    "\n",
    "        data_iter = tqdm(\n",
    "            enumerate(self.train_dataloader),\n",
    "            desc=\"EP_%s:%d\" % ('train', epoch),\n",
    "            total=len(self.train_dataloader),\n",
    "            bar_format=\"{l_bar}{r_bar}\"\n",
    "        )\n",
    "\n",
    "        avg_loss = 0.0\n",
    "        references = []  # true captions for calculating BLEU-4 score\n",
    "        hypotheses = []  # predictions\n",
    "        total_image_binary_acc = 0\n",
    "        image_binary_element = 0\n",
    "        binary_output_list = []\n",
    "        target_posi_nega_list = []\n",
    "\n",
    "        for i, batch in data_iter:\n",
    "            images_list, bert_output_id, captions_id, caption_lengths, dicom_file_num, posi_nega_list = batch\n",
    "            images = torch.tensor(\n",
    "                images_list, dtype=torch.float32).to(self.device)\n",
    "            bert_output_id = pad_sequence(\n",
    "                bert_output_id, batch_first=True).to(self.device)\n",
    "            caption_lengths = torch.tensor(caption_lengths).to(self.device)\n",
    "            posi_nega = torch.tensor(posi_nega_list).to(self.device)\n",
    "\n",
    "            # Encoder\n",
    "            encoder_out = self.cnn_encoder(images)\n",
    "\n",
    "            # Image Classification\n",
    "            classification_output = self.classification(encoder_out)\n",
    "\n",
    "            # Decoder\n",
    "            scores, caps_sorted, decode_lengths, sort_ind, alphas = self.decoder(\n",
    "                encoder_out,\n",
    "                bert_output_id,\n",
    "                captions_id,\n",
    "                caption_lengths,\n",
    "                posi_nega\n",
    "            )\n",
    "\n",
    "            scores_copy = scores.detach().clone()\n",
    "            \n",
    "            targets = caps_sorted[:, 1:]\n",
    "\n",
    "            scores = pack_padded_sequence(\n",
    "                scores,\n",
    "                decode_lengths,\n",
    "                batch_first=True\n",
    "            ).data.to(self.device)\n",
    "\n",
    "            targets = pack_padded_sequence(\n",
    "                targets,\n",
    "                decode_lengths,\n",
    "                batch_first=True\n",
    "            ).data.to(self.device)\n",
    "\n",
    "            encoder_loss = self.encoder_criterion(classification_output, posi_nega)\n",
    "            decoder_loss = self.decoder_criterion(scores, targets)\n",
    "\n",
    "            loss = encoder_loss + decoder_loss #+ last_n_token_decoder_loss\n",
    "\n",
    "            self.decoder_optimizer.zero_grad()\n",
    "            self.cnn_encoder_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.decoder_optimizer.step()\n",
    "            self.cnn_encoder_optimizer.step()\n",
    "\n",
    "            avg_loss += loss.item()\n",
    "\n",
    "            # references\n",
    "            img_caps_list = caps_sorted.tolist()\n",
    "            img_captions = [[[id2word(id_) for id_ in img_cap_list if id_ not in (\n",
    "                train_word_dic['<START>'], train_word_dic['<PAD>'])]] for img_cap_list in img_caps_list]\n",
    "            references.extend(img_captions)\n",
    "\n",
    "            # hypothesis\n",
    "            _, preds = torch.max(scores_copy, dim=2)\n",
    "            preds_list = preds.tolist()\n",
    "            temp_preds_list = []\n",
    "            for j in range(len(preds_list)):\n",
    "                temp_preds_list.append(\n",
    "                    [id2word(id_) for id_ in preds_list[j][:decode_lengths[j]]])\n",
    "            hypotheses.extend(temp_preds_list)\n",
    "\n",
    "            # image classification acc\n",
    "            image_binary_acc = classification_output.argmax(\n",
    "                dim=-1).eq(posi_nega).sum().item()\n",
    "            total_image_binary_acc += image_binary_acc\n",
    "            image_binary_element += posi_nega.nelement()\n",
    "            \n",
    "            # 画像分類の経過を観察\n",
    "            binary_output = classification_output.argmax(dim=-1).tolist()\n",
    "            binary_output_list.extend(binary_output)\n",
    "            target_posi_nega_list.extend(posi_nega.tolist())\n",
    "\n",
    "        bleu4 = corpus_bleu(\n",
    "            references,\n",
    "            hypotheses,\n",
    "            smoothing_function=SmoothingFunction().method7\n",
    "        )\n",
    "\n",
    "        avg_loss = avg_loss / len(data_iter)\n",
    "        total_image_binary_acc = total_image_binary_acc / image_binary_element\n",
    "        f1 = f1_score(target_posi_nega_list, binary_output_list)\n",
    "\n",
    "        self.save_loss_bleu(avg_loss, self.output_train_path, bleu4, total_image_binary_acc, f1)\n",
    "\n",
    "    def dev(self, epoch):\n",
    "\n",
    "        self.cnn_encoder.eval()\n",
    "        self.decoder.eval()\n",
    "\n",
    "        data_iter = tqdm(\n",
    "            enumerate(self.dev_dataloader),\n",
    "            desc=\"EP_%s:%d\" % ('dev', epoch),\n",
    "            total=len(self.dev_dataloader),\n",
    "            bar_format=\"{l_bar}{r_bar}\"\n",
    "        )\n",
    "\n",
    "        avg_loss = 0.0\n",
    "\n",
    "        references = []  # true captions for calculating BLEU-4 score\n",
    "        hypotheses = []  # predictions\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, batch in data_iter:\n",
    "                images_list, captions, captions_id, caption_lengths, dicom_file_num = batch\n",
    "                max_len = max(dicom_file_num)\n",
    "\n",
    "                images = torch.tensor(\n",
    "                    images_list, dtype=torch.float32).to(self.device)\n",
    "                caption_lengths = torch.tensor(caption_lengths).to(self.device)\n",
    "\n",
    "                encoder_out = self.cnn_encoder(images)\n",
    "                bert_outputs, bert_input_ids = self.bert_decoder(captions)\n",
    "                bert_last_hidden_state = bert_outputs.last_hidden_state\n",
    "\n",
    "                scores, caps_sorted, decode_lengths, sort_ind, alphas = self.decoder(\n",
    "                    encoder_out,\n",
    "                    bert_last_hidden_state,\n",
    "                    captions_id,\n",
    "                    caption_lengths,\n",
    "                    bert_input_ids\n",
    "                )\n",
    "\n",
    "                targets = caps_sorted[:, 1:]\n",
    "\n",
    "                scores_copy = scores.detach().clone()\n",
    "\n",
    "                scores = pack_padded_sequence(\n",
    "                    scores,\n",
    "                    decode_lengths,\n",
    "                    batch_first=True\n",
    "                ).data.to(self.device)\n",
    "\n",
    "                targets = pack_padded_sequence(\n",
    "                    targets,\n",
    "                    decode_lengths,\n",
    "                    batch_first=True\n",
    "                ).data.to(self.device)\n",
    "\n",
    "                loss = self.decoder_criterion(scores, targets)\n",
    "\n",
    "                avg_loss += loss.item()\n",
    "\n",
    "                # references\n",
    "                img_caps_list = caps_sorted.tolist()\n",
    "                img_captions = [[[id2word(id_) for id_ in img_cap_list if id_ not in (\n",
    "                    train_word_dic['<START>'], train_word_dic['<PAD>'])]] for img_cap_list in img_caps_list]\n",
    "                references.extend(img_captions)\n",
    "\n",
    "                # hypothesis\n",
    "                _, preds = torch.max(scores_copy, dim=2)\n",
    "                preds_list = preds.tolist()\n",
    "                temp_preds_list = []\n",
    "                for j in range(len(preds_list)):\n",
    "                    temp_preds_list.append(\n",
    "                        [id2word(id_) for id_ in preds_list[j][:decode_lengths[j]]])\n",
    "                hypotheses.extend(temp_preds_list)\n",
    "\n",
    "        avg_loss = avg_loss / len(data_iter)\n",
    "\n",
    "        bleu4 = corpus_bleu(\n",
    "            references,\n",
    "            hypotheses,\n",
    "            smoothing_function=SmoothingFunction().method7\n",
    "        )\n",
    "\n",
    "        self.save_loss_bleu(avg_loss, self.output_dev_path, bleu4)\n",
    "        return bleu4\n",
    "\n",
    "    def save_model(self, epoch, file_path=\"output/bert_trained.model\"):\n",
    "        \"\"\"\n",
    "        Saving the current BERT model on file_path\n",
    "        :param epoch: current epoch number\n",
    "        :param file_path: model output path which gonna be file_path+\"ep%d\" % epoch\n",
    "        :return: final_output_path\n",
    "        \"\"\"\n",
    "        output_path = file_path + \"ep%d\" % epoch\n",
    "        torch.save(self.cnn_encoder.to('cpu').state_dict(),\n",
    "                   output_path + '_cnn_encoder.bin')\n",
    "        torch.save(self.decoder.to('cpu').state_dict(),\n",
    "                   output_path + '_decoder.bin')\n",
    "        torch.save(self.classification.to('cpu').state_dict(),\n",
    "                   output_path + '_classification.bin')\n",
    "        self.cnn_encoder.to(self.device)\n",
    "        self.decoder.to(self.device)\n",
    "        self.classification.to(self.device)\n",
    "\n",
    "        return output_path\n",
    "\n",
    "#     pickleでの追加がないので，読み込んで，追加で書き込んで，もう一度保存する形で追加する\n",
    "    def save_loss_bleu(self, loss, output_path, bleu, image_binary_acc, f1):\n",
    "        loss_bleu_acc_dic = pickle_load(output_path)\n",
    "        loss_bleu_acc_dic['loss'].append(loss)\n",
    "        loss_bleu_acc_dic['bleu'].append(bleu)\n",
    "        loss_bleu_acc_dic['image_binary_acc'].append(image_binary_acc)\n",
    "        loss_bleu_acc_dic['f1'].append(f1)\n",
    "\n",
    "        pickle_dump(loss_bleu_acc_dic, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T08:02:51.749929Z",
     "start_time": "2022-03-07T08:02:51.239Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "trainer = CNN_BERT_LSTM_Trainer(\n",
    "    train_dataloader=train_data_loader,\n",
    "    with_cuda=with_cuda,\n",
    "    lr=lr,\n",
    "    betas=(adam_beta1, adam_beta2),\n",
    "    weight_decay=adam_weight_decay,\n",
    "    attention_dim=attention_dim,\n",
    "    embed_dim=bert_hidden_dim,\n",
    "    decoder_dim=decoder_dim,\n",
    "    decoder_vocab_size=len(train_word_dic),\n",
    "    encoder_dim=encoder_dim,\n",
    "    dropout=dropout,\n",
    "    output_path=output_path,\n",
    "    subword_num=total_subword_num,\n",
    "    model_depth=model_depth,\n",
    "    last_n_token=last_n_token,\n",
    "    cnn_pretrain_weight=cnn_pretrain_weight\n",
    ")\n",
    "print('START')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T08:02:51.750794Z",
     "start_time": "2022-03-07T08:02:51.244Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_bleu = 0.0\n",
    "for epoch in range(EPOCHS):\n",
    "    trainer.train(epoch)\n",
    "    trainer.save_model(epoch, output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
